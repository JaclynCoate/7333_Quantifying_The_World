---
output:
  pdf_document: default
  html_document: default
---
--
title: "Case Study: Spam Email Prediction"
author: "Jaclyn Coate, Megan Riley, Reagan Meagher"
date: "2/13/2021"
output: pdf_document
---

# 1. Introduction

The digital world is something that has increasingly changed the way organizations and businesses communicate with their consumers. As digital memory becomes more cost effective it takes no effort at all for a company to render and send large volumes of emails in an attempt to engage their end users. While is a positive shift for companies in their effort to grow their business, consumers are inundated with spam emails that are problematic for those who use their email for other purposes than just shopping. Individuals can easily spot their spam emails and skip or delete them. But this manual effort can be strenuous and take a long time based on sheer volume of spam emails alone. This may not be a big issue for the casual email users who only use it for online shopping and occasional correspondence. But the individuals who live in the digital world would much rather not spend their time sorting email, but rather leveraging their email for their end goals.

This leads us to the case study we present today; below are our efforts to make sure we have a strong spam email filter to take the burden off of email users for manually filtering their spam emails out. The first question is how to apply a filter that effectively filters spam emails. Our process for an automatic procedure closely resembles the process a human might go through in order to determine if their email is spam. We review the _subject line_ and if we can’t tell from that we examine the _body_ without ever opening an _attachment_ in case of a virus. With cleaning and examining these different elements of an email programmatically we can leverage extremely powerful machine learning algorithmic classifiers such as: _NaiveBayes_, _Decisions Tree Classifier_, and _XGBoost_/_Support Vector Machine_.

```{r environment set up, echo=FALSE, results='markup', warning=FALSE}
#install.packages("tm")
library(tm)
#install.packages("naivebayes")
library(naivebayes)
#install.packages("e1071")
library(e1071)
#install.packages("rpart")
library(rpart)
#install.packages("randomForest")
library(randomForest)
#install.packages("xgboost")
library(xgboost)
#install.packages("caret")
library(caret)
library(MLmetrics)
library(RColorBrewer)
library(tidyverse)
Sys.setlocale("LC_ALL", "pt_PT.ISO8859-1")
```
# 2. Data Cleaning 

## 2.1 Background 

These data for this case study consisted of 9000 emails that have been hand classified into spam and non-spam categories, the non-spam categories referred to as ham. This was done my [Apache SpamAssassin](https://spamassassin.apache.org/), the premier open source platform for spam detection. We obtained this dataset through the [Public Corpus](https://spamassassin.apache.org/old/publiccorpus/) generated by the Apache SpamAssassin. 

The data are classified as ham(non-spam) or spam emails. The ham training data is split into two types; hard ham and easy ham. Hard ham are still non-spam emails however they carry some of the same characteristics as spam emails and are more difficult to classify. 

Overall it is important to note as we build models for the purpose of classifying spam emails from this corpus of data that the emails themselves are at minimum nearly fifteen years old. Spam techniques along with online communication can change drastically in a year, much less fifteen years. It is most likely any models or techniques built from this corpus of emails will have to be changed and edited in the future for any further potential use at spam classification. 


To begin the exploration of the data we begin the process of cleaning and working with the data below to explore the distribution of words and otehr aspects of the messages. 

```{r }
splitMessage = function(msg) {
  splitPoint = match("", msg)
  header = msg[1:(splitPoint-1)]
  body = msg[ -(1:splitPoint) ]
  return(list(header = header, body = body))
}
getBoundary = function(header) {
  boundaryIdx = grep("boundary=", header)
  boundary = gsub('"', "", header[boundaryIdx])
  gsub(".*boundary= *([^;]*);?.*", "\\1", boundary)
}
dropAttach = function(body, boundary){
  
  bString = paste("--", boundary, sep = "")
  bStringLocs = which(bString == body)
  
  if (length(bStringLocs) <= 1) return(body)
  
  eString = paste("--", boundary, "--", sep = "")
  eStringLoc = which(eString == body)
  if (length(eStringLoc) == 0) 
    return(body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1)])
  
  n = length(body)
  if (eStringLoc < n) 
     return( body[ c( (bStringLocs[1] + 1) : (bStringLocs[2] - 1), 
                    ( (eStringLoc + 1) : n )) ] )
  
  return( body[ (bStringLocs[1] + 1) : (bStringLocs[2] - 1) ])
}
findMsgWords = 
function(msg, stopWords) {
 if(is.null(msg))
  return(character())
 words = unique(unlist(strsplit(cleanText(msg), "[[:blank:]\t]+")))
 
 # drop empty and 1 letter words
 words = words[ nchar(words) > 1]
 words = words[ !( words %in% stopWords) ]
 invisible(words)
}
cleanText =
function(msg)   {
  tolower(gsub("[[:punct:]0-9[:space:][:blank:]]+", " ", msg))
}
processAllWords = function(dirName, stopWords)
{
       # read all files in the directory
  fileNames = list.files(dirName, full.names = TRUE)
       # drop files that are not email, i.e., cmds
  notEmail = grep("cmds$", fileNames)
  if ( length(notEmail) > 0) fileNames = fileNames[ - notEmail ]
  messages = lapply(fileNames, readLines, encoding = "latin1")
  
       # split header and body
  emailSplit = lapply(messages, splitMessage)
       # put body and header in own lists
  bodyList = lapply(emailSplit, function(msg) msg$body)
  headerList = lapply(emailSplit, function(msg) msg$header)
  rm(emailSplit)
  
       # determine which messages have attachments
  hasAttach = sapply(headerList, function(header) {
    CTloc = grep("Content-Type", header)
    if (length(CTloc) == 0) return(0)
    multi = grep("multi", tolower(header[CTloc])) 
    if (length(multi) == 0) return(0)
    multi
  })
  
  hasAttach = which(hasAttach > 0)
  
       # find boundary strings for messages with attachments
  boundaries = sapply(headerList[hasAttach], getBoundary)
  
       # drop attachments from message body
  bodyList[hasAttach] = mapply(dropAttach, bodyList[hasAttach], 
                               boundaries, SIMPLIFY = FALSE)
  
       # extract words from body
  msgWordsList = lapply(bodyList, findMsgWords, stopWords)
  
  invisible(msgWordsList)
}
```

Below we build a directory path locally on one machine, with the overall directory, messages can be processed by the previous functions. The stop words of interest are also cleaned and set up using a general stop word lexicon for this process.  

```{r}
#Directory paths
spamPath = 
  "/Users/zmartygirl/githubrepos1/7333_Quantifying_The_World/Unit6_CaseStudy3/data"
dirNames = list.files(path = paste(spamPath, sep = .Platform$file.sep))
fullDirNames = paste(spamPath, dirNames, sep = .Platform$file.sep)
#Set up Stop Words
stopWords = stopwords()
cleanSW = tolower(gsub("[[:punct:]0-9[:blank:]]+", " ", stopWords))
SWords = unlist(strsplit(cleanSW, "[[:blank:]]+"))
SWords = SWords[ nchar(SWords) > 1 ]
stopWords = unique(SWords)
```

With preparations done, we can load all the available messages into our session and clean and organize them into a set of words for each message. The entirety of this happens within the _processAllWords_ function. We can see we have over nine thousand messages, with 5051, 1400, and 500 in separate directories of non spam emails and 998 and 1396 emails in the spam category. 

```{r, warning=FALSE, echo=F}
# Grabbing all valide words from all emails
msgWordsList = lapply(fullDirNames, processAllWords, 
                      stopWords = stopWords)
numMsgs = sapply(msgWordsList, length)
numMsgs
```

We then add a label of spam or not spam to every message in our spam corpus. We know our first three directories are not spam and our final two are spam, so they are labeled as such below. 

```{r}
#Below we are adding the true labels on if messagse are spam or not spam. This is our known determination because it was hand labeled.
isSpam = rep(c(FALSE, FALSE, FALSE, TRUE, TRUE), numMsgs)
msgWordsList = unlist(msgWordsList, recursive = FALSE)
```

Next we are building an index of messages and train and test split of the messages and classification labels. We also can see that there are eighty thousand unique words in the set of training data given by the length of our bag of words.

```{r, echo=F, include=F}
numEmail = length(isSpam)
numSpam = sum(isSpam)
numHam = numEmail - numSpam

set.seed(418910)
#Test / Train Split
testSpamIdx = sample(numSpam, size = floor(numSpam/3))
testHamIdx = sample(numHam, size = floor(numHam/3))
#Split out into test and train data
testMsgWords = c((msgWordsList[isSpam])[testSpamIdx],
                 (msgWordsList[!isSpam])[testHamIdx] )
trainMsgWords = c((msgWordsList[isSpam])[ - testSpamIdx], 
                  (msgWordsList[!isSpam])[ - testHamIdx])
testIsSpam = rep(c(TRUE, FALSE), 
                 c(length(testSpamIdx), length(testHamIdx)))
trainIsSpam = rep(c(TRUE, FALSE), 
                 c(numSpam - length(testSpamIdx), 
                   numHam - length(testHamIdx)))

# the number of unique words in the training data
bow = unique(unlist(trainMsgWords))
length(bow)

# the number of occurances of the unique words in a spam message
spamWordCounts = rep(0, length(bow))
names(spamWordCounts) = bow
tmp = lapply(trainMsgWords[trainIsSpam], unique)
tt = table( unlist(tmp) )
spamWordCounts[ names(tt) ] = tt

```


```{r, echo=F, include=F}


# number of occurrences of spam and non-spam for each word on the list
computeFreqs =
function(wordsList, spam, bow = unique(unlist(wordsList)))
{
   # create a matrix for spam, ham, and log odds
  wordTable = matrix(0.5, nrow = 4, ncol = length(bow), 
                     dimnames = list(c("spam", "ham", 
                                        "presentLogOdds", 
                                        "absentLogOdds"),  bow))
   # For each spam message, add 1 to counts for words in message
  counts.spam = table(unlist(lapply(wordsList[spam], unique)))
  wordTable["spam", names(counts.spam)] = counts.spam + .5
   # Similarly for ham messages
  counts.ham = table(unlist(lapply(wordsList[!spam], unique)))  
  wordTable["ham", names(counts.ham)] = counts.ham + .5  
   # Find the total number of spam and ham
  numSpam = sum(spam)
  numHam = length(spam) - numSpam
   # Prob(word|spam) and Prob(word | ham)
  wordTable["spam", ] = wordTable["spam", ]/(numSpam + .5)
  wordTable["ham", ] = wordTable["ham", ]/(numHam + .5)
  
   # log odds
  wordTable["presentLogOdds", ] = 
     log(wordTable["spam",]) - log(wordTable["ham", ])
  wordTable["absentLogOdds", ] = 
     log((1 - wordTable["spam", ])) - 
    log((1 -wordTable["ham", ]))
  invisible(wordTable)
}

# applying spam versus non-spam function to training data
trainTable = computeFreqs(trainMsgWords, trainIsSpam)
```

Below we are creating a classification test dataset. There is potential for no words in the training data. Below we are calculating the log odds without new words.

```{r, echo=F}
# Note: the first message is spac, and the log likelihood is positive
newMsg = testMsgWords[[1]]
newMsg = newMsg[!is.na(match(newMsg, colnames(trainTable)))]
present = colnames(trainTable) %in% newMsg
sum(trainTable["presentLogOdds", present]) + 
  sum(trainTable["absentLogOdds", !present])

# Note: A negative value with a large log likelihood ratio indiciates a non-spam message
newMsg = testMsgWords[[ which(!testIsSpam)[1] ]]
newMsg = newMsg[!is.na(match(newMsg, colnames(trainTable)))]
present = (colnames(trainTable) %in% newMsg)
sum(trainTable["presentLogOdds", present]) + 
     sum(trainTable["absentLogOdds", !present])

# function to get log likelihood
computeMsgLLR = function(words, freqTable) 
{
       # Discards words not in training data.
  words = words[!is.na(match(words, colnames(freqTable)))]
       # Find which words are present
  present = colnames(freqTable) %in% words
  sum(freqTable["presentLogOdds", present]) +
    sum(freqTable["absentLogOdds", !present])
}
```

Below we have displayed the distributions of the log likelihoods for each of the different classes: _spam_ versus _non-spam_. Due to the distribution spread of the Spam email messages being nearest zero (including the mean) we can state that the data shows our probability is more accurate at identifying the Spam versus the Non-spam messages. We also noted that below the quartile ranges of the _non-spam_ emails is much smaller than the spam emails. This tells us that the repeatability of results for identifying _non-spam_ messages is higher than the Spam messages. Simply stated this indicates, the words unique words identified in _non-spam_ messages are not as unique to _non-spam_ messages as those words used in Spam messages. Additionally, the words most used in _non-spam_ messages can also appear in _spam messages.

Analyzing from a logical perspective we can note that _spam_ emails often contain words that might be difficult for detectors to pick up, such as repetitive letters to change the n_grams of the words and _leet speak_. _Leet speak_ is the practice of replacing words with letters and such as an _E_ becomes a _3_ or an _0_ becomse a _0_.



```{r}
# do not run twice
testLLR = sapply(testMsgWords, computeMsgLLR, trainTable)
tapply(testLLR, testIsSpam, summary)
# boxplots of log likelihoods
#pdf("SP_Boxplot.pdf", width = 6, height = 6)
spamLab = c("ham", "spam")[1 + testIsSpam]
boxplot(testLLR ~ spamLab, 
        ylab = "Log Likelihood Ratio",
       main = "Log Likelihood Ratio for Randomly Chosen Test Messages",
        ylim=c(-500, 500))
#dev.off()
```

Below this code demonstrates misclassifications and calculates type I error rates with a function given the logged likelihood values and true data.

```{r}
typeIErrorRate = 
function(tau, llrVals, spam)
{
  classify = llrVals > tau
  sum(classify & !spam)/sum(!spam)
}
#tau 0
typeIErrorRate(0, testLLR,testIsSpam)
#tau -20
typeIErrorRate(-20, testLLR,testIsSpam)
typeIErrorRates = 
function(llrVals, isSpam) 
{
  o = order(llrVals)
  llrVals =  llrVals[o]
  isSpam = isSpam[o]
  idx = which(!isSpam)
  N = length(idx)
  list(error = (N:1)/N, values = llrVals[idx])
}
```

Below this code demonstrates misclassifications and calculates type II error rates with a function given the logged likelihood values and true data. 

```{r}
typeIIErrorRates = function(llrVals, isSpam) {
    
  o = order(llrVals)
  llrVals =  llrVals[o]
  isSpam = isSpam[o]
    
    
  idx = which(isSpam)
  N = length(idx)
  list(error = (1:(N))/N, values = llrVals[idx])
  }  
xI = typeIErrorRates(testLLR, testIsSpam)
xII = typeIIErrorRates(testLLR, testIsSpam)
tau01 = round(min(xI$values[xI$error <= 0.01]))
t2 = max(xII$error[ xII$values < tau01 ])
```

## 2.2 

### Type I versus Type II Error Rates and Log-Likelihood Thresholds

Type I error rate is also known as a false positive error rate and occurs when the predictor incorrectly rejects a true null hypothesis. In our research the null hypothesis states that a a message is not spam. Below, in Figure XX, we have displayed the distribution of the Type I and Type II errors of our log likelihood values. Type I error rate is also known as a false positive error rate and occurs when the predictor incorrectly rejects a true null hypothesis. Type II error rate is also known as a false negative error rate and occurs when the predictor incorrectly accepts a false null hypothesis. As indicated by the orange marker, the threshold for determining spam is a log likelihood value of -41. Otherwise stated, any message that has a log likelihood value that is greater than -41 would be classified as a spam email and any message that has a log-likelihood of less than -41 is not a spam email.

```{r}
#pdf("LinePlotTypeI+IIErrors.pdf", width = 8, height = 6)

cols = brewer.pal(9, "Set1")[c(3, 4, 5)]
plot(xII$error ~ xII$values,  type = "l", col = cols[1], lwd = 3,
     xlim = c(-300, 250), ylim = c(0, 1),
     xlab = "Log Likelihood Ratio Values", ylab="Error Rate", 
     main = "Type I & Type II Error Rates: Spam vs Non-spam Log Likelihood Ratios")
points(xI$error ~ xI$values, type = "l", col = cols[2], lwd = 3)
legend(x = 50, y = 0.4, fill = c(cols[2], cols[1]),
       legend = c("Classify Ham as Spam", 
                  "Classify Spam as Ham"), cex = 0.8,
       bty = "n")
abline(h=0.01, col ="grey", lwd = 3, lty = 2)
text(-250, 0.05, pos = 4, "Type I Error = 0.01", col = cols[2])
mtext(tau01, side = 1, line = 0.5, at = tau01, col = cols[3])
segments(x0 = tau01, y0 = -.50, x1 = tau01, y1 = t2, 
         lwd = 2, col = "grey")
text(tau01 + 20, 0.05, pos = 4,
     paste("Type II Error = ", round(t2, digits = 2)), 
     col = cols[1])
```

NOTE: In the above figure the Type I error threshold is 0.01 and the threshold for Type II error rate is 0.04.

```{r}
#Below we are obtaining a tau so that one type of error is withint 1% using our 5-fold cross-validation.
#dev.off()
k = 5
numTrain = length(trainMsgWords)
partK = sample(numTrain)
tot = k * floor(numTrain/k)
partK = matrix(partK[1:tot], ncol = k)
testFoldOdds = NULL
for (i in 1:k) {
  foldIdx = partK[ , i]
  trainTabFold = computeFreqs(trainMsgWords[-foldIdx], trainIsSpam[-foldIdx])
  testFoldOdds = c(testFoldOdds, 
               sapply(trainMsgWords[ foldIdx ], computeMsgLLR, trainTabFold))
}
testFoldSpam = NULL
for (i in 1:k) {
  foldIdx = partK[ , i]
  testFoldSpam = c(testFoldSpam, trainIsSpam[foldIdx])
}
xFoldI = typeIErrorRates(testFoldOdds, testFoldSpam)
xFoldII = typeIIErrorRates(testFoldOdds, testFoldSpam)
tauFoldI = round(min(xFoldI$values[xFoldI$error <= 0.01]))
tFold2 = xFoldII$error[ xFoldII$values < tauFoldI ]
```

## 2.3 Feature Creation 

Borrowing off of the provided exploration of the messages we used code provided to build a thurough list of features and created dataset to run models with. 

In the below chunk hidden from written output we build a list of functions used to create a derived dataset. This includes facotrs such as if the email uses excessive capitalization or if it is a reply to someone, a forward, includes underscors or a dear sir or madam address. All of these factors are fairly simple derivations of the pure messages and able to better describe some unique choices made by the email authors that may give hints as to whether they are spam or ham emails. 

```{r function list, include = FALSE}

funcList = list(
  isSpam =
    expression(msg$isSpam)
  ,
  isRe =
    function(msg) {
      # Can have a Fwd: Re:  ... but we are not looking for this here.
      # We may want to look at In-Reply-To field.
      "Subject" %in% names(msg$header) && 
        length(grep("^[ \t]*Re:", msg$header[["Subject"]])) > 0
    }
  ,
  numLines =
    function(msg) length(msg$body)
  ,
  bodyCharCt =
    function(msg)
      sum(nchar(msg$body))
  ,
  underscore =
    function(msg) {
      if(!"Reply-To" %in% names(msg$header))
        return(FALSE)
      
      txt <- msg$header[["Reply-To"]]
      length(grep("_", txt)) > 0  && 
        length(grep("[0-9A-Za-z]+", txt)) > 0
    }
  ,
  subExcCt = 
    function(msg) {
      x = msg$header["Subject"]
      if(length(x) == 0 || sum(nchar(x)) == 0 || is.na(x))
        return(NA)
      
      sum(nchar(gsub("[^!]","", x)))
    }
  ,
  subQuesCt =
    function(msg) {
      x = msg$header["Subject"]
      if(length(x) == 0 || sum(nchar(x)) == 0 || is.na(x))
        return(NA)
      
      sum(nchar(gsub("[^?]","", x)))
    }
  ,
  numAtt = 
    function(msg) {
      if (is.null(msg$attach)) return(0)
      else nrow(msg$attach)
    }
   
  ,
  priority =
    function(msg) {
      ans <- FALSE
      # Look for names X-Priority, Priority, X-Msmail-Priority
      # Look for high any where in the value
      ind = grep("priority", tolower(names(msg$header)))
      if (length(ind) > 0)  {
        ans <- length(grep("high", tolower(msg$header[ind]))) >0
      }
      ans
    }
  ,
  numRec =
    function(msg) {
      # unique or not.
      els = getMessageRecipients(msg$header)
      
      if(length(els) == 0)
        return(NA)
      
      # Split each line by ","  and in each of these elements, look for
      # the @ sign. This handles
      tmp = sapply(strsplit(els, ","), function(x) grep("@", x))
      sum(sapply(tmp, length))
    }
  ,
  perCaps =
    function(msg)
    {
      body = paste(msg$body, collapse = "")
      
      # Return NA if the body of the message is "empty"
      if(length(body) == 0 || nchar(body) == 0) return(NA)
      
      # Eliminate non-alpha characters and empty lines 
      body = gsub("[^[:alpha:]]", "", body)
      els = unlist(strsplit(body, ""))
      ctCap = sum(els %in% LETTERS)
      100 * ctCap / length(els)
    }
  ,
  isInReplyTo =
    function(msg)
    {
      "In-Reply-To" %in% names(msg$header)
    }
  ,
  sortedRec =
    function(msg)
    {
      ids = getMessageRecipients(msg$header)
      all(sort(ids) == ids)
    }
  ,
  subPunc =
    function(msg)
    {
      if("Subject" %in% names(msg$header)) {
        el = gsub("['/.:@-]", "", msg$header["Subject"])
        length(grep("[A-Za-z][[:punct:]]+[A-Za-z]", el)) > 0
      }
      else
        FALSE
    },
  hour =
    function(msg)
    {
      date = msg$header["Date"]
      if ( is.null(date) ) return(NA)
      # Need to handle that there may be only one digit in the hour
      locate = regexpr("[0-2]?[0-9]:[0-5][0-9]:[0-5][0-9]", date)
      
      if (locate < 0)
        locate = regexpr("[0-2]?[0-9]:[0-5][0-9]", date)
      if (locate < 0) return(NA)
      
      hour = substring(date, locate, locate+1)
      hour = as.numeric(gsub(":", "", hour))
      
      locate = regexpr("PM", date)
      if (locate > 0) hour = hour + 12
      
      locate = regexpr("[+-][0-2][0-9]00", date)
      if (locate < 0) offset = 0
      else offset = as.numeric(substring(date, locate, locate + 2))
      (hour - offset) %% 24
    }
  ,
  multipartText =
    function(msg)
    {
      if (is.null(msg$attach)) return(FALSE)
      numAtt = nrow(msg$attach)
      
      types = 
        length(grep("(html|plain|text)", msg$attach$aType)) > (numAtt/2)
    }
  ,
  hasImages =
    function(msg)
    {
      if (is.null(msg$attach)) return(FALSE)
      
      length(grep("^ *image", tolower(msg$attach$aType))) > 0
    }
  ,
  isPGPsigned =
    function(msg)
    {
      if (is.null(msg$attach)) return(FALSE)
      
      length(grep("pgp", tolower(msg$attach$aType))) > 0
    },
  perHTML =
    function(msg)
    {
      if(! ("Content-Type" %in% names(msg$header))) return(0)
      
      el = tolower(msg$header["Content-Type"]) 
      if (length(grep("html", el)) == 0) return(0)
      
      els = gsub("[[:space:]]", "", msg$body)
      totchar = sum(nchar(els))
      totplain = sum(nchar(gsub("<[^<]+>", "", els )))
      100 * (totchar - totplain)/totchar
    },
  subSpamWords =
    function(msg)
    {
      if("Subject" %in% names(msg$header))
        length(grep(paste(SpamCheckWords, collapse = "|"), 
                    tolower(msg$header["Subject"]))) > 0
      else
        NA
    }
  ,
  subBlanks =
    function(msg)
    {
      if("Subject" %in% names(msg$header)) {
        x = msg$header["Subject"]
        # should we count blank subject line as 0 or 1 or NA?
        if (nchar(x) == 1) return(0)
        else 100 *(1 - (nchar(gsub("[[:blank:]]", "", x))/nchar(x)))
      } else NA
    }
  ,
  noHost =
    function(msg)
    {
      # Or use partial matching.
      idx = pmatch("Message-", names(msg$header))
      
      if(is.na(idx)) return(NA)
      
      tmp = msg$header[idx]
      return(length(grep(".*@[^[:space:]]+", tmp)) ==  0)
    }
  ,
  numEnd =
    function(msg)
    {
      # If we just do a grep("[0-9]@",  )
      # we get matches on messages that have a From something like
      # " \"marty66@aol.com\" <synjan@ecis.com>"
      # and the marty66 is the "user's name" not the login
      # So we can be more precise if we want.
      x = names(msg$header)
      if ( !( "From" %in% x) ) return(NA)
      login = gsub("^.*<", "", msg$header["From"])
      if ( is.null(login) ) 
        login = gsub("^.*<", "", msg$header["X-From"])
      if ( is.null(login) ) return(NA)
      login = strsplit(login, "@")[[1]][1]
      length(grep("[0-9]+$", login)) > 0
    },
  isYelling =
    function(msg)
    {
      if ( "Subject" %in% names(msg$header) ) {
        el = gsub("[^[:alpha:]]", "", msg$header["Subject"])
        if (nchar(el) > 0) nchar(gsub("[A-Z]", "", el)) < 1
        else FALSE
      }
      else
        NA
    },
  forwards =
    function(msg)
    {
      x = msg$body
      if(length(x) == 0 || sum(nchar(x)) == 0)
        return(NA)
      
      ans = length(grep("^[[:space:]]*>", x))
      100 * ans / length(x)
    },
  isOrigMsg =
    function(msg)
    {
      x = msg$body
      if(length(x) == 0) return(NA)
      
      length(grep("^[^[:alpha:]]*original[^[:alpha:]]+message[^[:alpha:]]*$", 
                  tolower(x) ) ) > 0
    },
  isDear =
    function(msg)
    {
      x = msg$body
      if(length(x) == 0) return(NA)
      
      length(grep("^[[:blank:]]*dear +(sir|madam)\\>", 
                  tolower(x))) > 0
    },
  isWrote =
    function(msg)
    {
      x = msg$body
      if(length(x) == 0) return(NA)
      
      length(grep("(wrote|schrieb|ecrit|escribe):", tolower(x) )) > 0
    },
  avgWordLen =
    function(msg)
    {
      txt = paste(msg$body, collapse = " ")
      if(length(txt) == 0 || sum(nchar(txt)) == 0) return(0)
      
      txt = gsub("[^[:alpha:]]", " ", txt)
      words = unlist(strsplit(txt, "[[:blank:]]+"))
      wordLens = nchar(words)
      mean(wordLens[ wordLens > 0 ])
    }
  ,
  numDlr =
    function(msg)
    {
      x = paste(msg$body, collapse = "")
      if(length(x) == 0 || sum(nchar(x)) == 0)
        return(NA)
      
      nchar(gsub("[^$]","", x))
    }
)

```

```{r Re Process Emails}

readEmail = function(dirName) {
       # retrieve the names of files in directory
  fileNames = list.files(dirName, full.names = TRUE)
       # drop files that are not email
  notEmail = grep("cmds$", fileNames)
  if ( length(notEmail) > 0) fileNames = fileNames[ - notEmail ]
       # read all files in the directory
  lapply(fileNames, readLines, encoding = "latin1")
}

processHeader = function(header)
{
       # modify the first line to create a key:value pair
  header[1] = sub("^From", "Top-From:", header[1])
  
  headerMat = read.dcf(textConnection(header), all = TRUE)
  headerVec = unlist(headerMat)
  
  dupKeys = sapply(headerMat, function(x) length(unlist(x)))
  names(headerVec) = rep(colnames(headerMat), dupKeys)
  
  return(headerVec)
}

processAttach = function(body, contentType){
  
  boundary = getBoundary(contentType)
  
  bString = paste("--", boundary, "$", sep = "")
  bStringLocs = grep(bString, body)
  
  eString = paste("--", boundary, "--$", sep = "")
  eStringLoc = grep(eString, body)
  
  n = length(body)
  
  if (length(eStringLoc) == 0) eStringLoc = n + 1
  if (length(bStringLocs) == 1) attachLocs = NULL
  else attachLocs = c(bStringLocs[-1],  eStringLoc)
  msg = body[ (bStringLocs[1] + 1) : min(n, (bStringLocs[2] - 1), na.rm = TRUE)]
  
  if ( eStringLoc < n )
    msg = c(msg, body[ (eStringLoc + 1) : n ])
  
  if ( !is.null(attachLocs) ) {
    attachLens = diff(attachLocs, lag = 1) 
    attachTypes = mapply(function(begL, endL) {
      contentTypeLoc = grep("[Cc]ontent-[Tt]ype", body[ (begL + 1) : (endL - 1)])
      contentType = body[ begL + contentTypeLoc]
      contentType = gsub('"', "", contentType )
      MIMEType = sub(" *Content-Type: *([^;]*);?.*", "\\1", contentType)
      return(MIMEType)
    }, attachLocs[-length(attachLocs)], attachLocs[-1])
  }
  
  if (is.null(attachLocs)) return(list(body = msg, attachInfo = NULL) )
  else return(list(body = msg, 
                   attachDF = data.frame(aLen = attachLens, 
                                         aType = attachTypes,
                                         stringsAsFactors = FALSE)))                                
}

getMessageRecipients =
  function(header)
  {
    c(if("To" %in% names(header))  header[["To"]] else character(0),
      if("Cc" %in% names(header))  header[["Cc"]] else character(0),
      if("Bcc" %in% names(header)) header[["Bcc"]] else character(0)
    )
  }


processAllEmail = function(dirName, isSpam = FALSE)
{
       # read all files in the directory
  messages = readEmail(dirName)
  fileNames = names(messages)
  n = length(messages)
  
       # split header from body
  eSplit = lapply(messages, splitMessage)
  rm(messages)
       # process header as named character vector
  headerList = lapply(eSplit, function(msg) 
                                 processHeader(msg$header))
  
       # extract content-type key
  contentTypes = sapply(headerList, function(header) 
                                       header["Content-Type"])
  
       # extract the body
  bodyList = lapply(eSplit, function(msg) msg$body)
  rm(eSplit)

    hasAttach = sapply(headerList, function(header) {
    CTloc = grep("Content-Type", header)
    if (length(CTloc) == 0) return(0)
    multi = grep("multi", tolower(header[CTloc])) 
    if (length(multi) == 0) return(0)
    multi
  })
  
  hasAttach = which(hasAttach > 0)
  
       # find boundary strings for messages with attachments
  boundaries = sapply(headerList[hasAttach], getBoundary)
  
       # drop attachments from message body
  bodyList[hasAttach] = mapply(dropAttach, bodyList[hasAttach], 
                               boundaries, SIMPLIFY = FALSE)
  
  emailList = mapply(function(header, body, isSpam) {
                       list(isSpam = isSpam, header = header, 
                            body = body)
                     },
                     headerList, bodyList, 
                     rep(isSpam, n), SIMPLIFY = FALSE )
  names(emailList) = fileNames
  
  invisible(emailList)
}
emailStruct = mapply(processAllEmail, fullDirNames, isSpam = rep( c(FALSE, TRUE), 3:2))      
emailStruct = unlist(emailStruct, recursive = FALSE)



```

```{r}
createDerivedDF =
function(email = emailStruct, operations = funcList, 
         verbose = FALSE)
{
  els = lapply(names(operations),
               function(id) {
                 if(verbose) print(id)
                 e = operations[[id]]
                 v = if(is.function(e)) 
                        sapply(email, e)
                      else 
                        sapply(email, function(msg) eval(e))
                 v
         })
   df = as.data.frame(els)
   names(df) = names(operations)
   invisible(df)
}

SpamCheckWords =
  c("viagra", "pounds", "free", "weight", "guarantee", "million", 
    "dollars", "credit", "risk", "prescription", "generic", "drug",
    "financial", "save", "dollar", "erotic", "million", "barrister",
    "beneficiary", "easy", 
    "money back", "money", "credit card")


emailDF = createDerivedDF(emailStruct)


```

# 3. Models 

We constructed four different models in order to determine the best classification technique for spam emails: _naive bayes_, _partition trees_,  _random forest_, and _XGBoost_. Note, we chose to use those classification types that could handle nonlinear relationships between the data. Regularization is not required in _naive bayes _or _partition tree_ analysis. They do however require parameters to hyper-tune. We have split the data into a test and train groups. In addition, we have performed a 5-fold cross-validation on the training data. Then leveraging an F1 score as the selection criteria for the best hypertuned parameter.


Below we run with minimal output the creation of several helper functions that will enable us to smoothly run each model. 

```{r}

setupRnum = function(data) {
  logicalVars = which(sapply(data, is.logical))
  facVars = lapply(data[ , logicalVars], 
                   function(x) {
                      x = as.numeric(x)
                   })
  cbind(facVars, data[ , - logicalVars])
}

emailDFnum = setupRnum(emailDF)
emailDFnum[is.na(emailDFnum)]<-0
cv_folds <- createFolds(emailDFnum$isSpam, k=5, list=TRUE, 
                        returnTrain = TRUE)
#Folds should be equal in length about 7500

#Sets up F1 measurement
f1 <- function(data, lev = NULL, model = NULL) {
  f1_val <- F1_Score(y_pred = data$pred, y_true = data$obs, positive = lev[1])
  p <- Precision(y_pred = data$pred, y_true = data$obs, positive = lev[1])
  r <- Recall(y_pred = data$pred, y_true = data$obs, positive = lev[1])
  fp <-sum(data$pred==0 & data$obs==1)/length(data$pred)  
 
  fn <-sum(data$pred==1 & data$obs==0)/length(data$pred)
    c(F1 = f1_val,
    prec = p,
    rec = r,
    Type_I_err=fp,
    Type_II_err=fn
   )
}
```



## Naive Bayes Model


Since the _naive bayes_ approach cannot easily classify these nuanced characteristics we perform a _decision tree classifier_ and an _XGBoost_ analysis to compare results.


##### List 3.1: Naive Bayes Hyper Parameters
Laplace: smoothing parameter
Usekernel: is this is true we should use the _kernel density_ estimate, if this is false we should use the _gaussian density_ estimate.
Adjust: if this is true we should allow bandwidth of _kernel density_ estimate to be adjusted as needed, if false; do not allow as much flexibility with estimate adjustments.

##### Table 3.1: Hyperparameter Settings for Naive Bayes Model
Hyperparameter          |     Setting   |    F1
----------------------    | ------------------- | ----------------------   
Laplace            | 0        | 0.9197
Usekernel        | False        | 
Adjust              | False        | 

```{r, warnings = FALSE, message=FALSE}
set.seed(1263)
nb_grid<-expand.grid(laplace=c(0,0.1,0.3,0.5,1), 
                     usekernel=c(T,F), adjust=c(T,F))
train_control<-trainControl(method="cv", number=5, 
                            savePredictions = 'final',
                            summaryFunction = f1)
model_nb<-caret::train(as.factor(isSpam) ~ .,
                       data=emailDFnum, trControl = train_control, 
                       method='naive_bayes',tuneGrid = nb_grid)
model_nb

cmdf = model_nb$pred
nb_cm = confusionMatrix(cmdf$pred,cmdf$obs)

```
## Decision Trees and RandomForest Models

### 3.b Decision Tree & Random Forest Classifier

In order to fully determine how thoroughly a decision tree model can approach classification of email spam we used both a singular decision tree model using the _rpart_ toolset. This with a combination of a more thorough random forest that encompasses a wider random set of potential partition trees with one selected partition tree. Accordingly we expect and do see better results from a random forest approach. 
 
#### 3.b.i Decision Trees

Decision Tree models are simple tree-based model. This builds a singular tree based on categorical choices that then categorize each item in the dataset into tighter categories with probabilities indicative of class choice. A singular decision tree should be carefully chosen and pruned in order to build a generalizable model. Decision trees as a rule can be easily overfit to our training data, as we could create a specific tree deeply complex that correctly classifies each element in a training dataset. Therefore our work in choosing hyperparameters is essential to building a decision tree that does not overfit our data. The list below, _List 3.2: Decision Tree Hyper Parameter_ contains the hyperparameters we tune towards a best F1 score.  

Through a cross validated testing run we found the best value of Cp was .0005 with a F1 score of __.9608__

##### List 3.2: Decision Tree Hyper Parameter
Cp: decrease in the lack of fit for any new split


```{r}
val<-seq(from = 0, to=0.01, by=0.0005)
cart_grid<-expand.grid(cp=val)
train_control<-trainControl(method="cv", number =5, 
                            savePredictions = 'final',
                            summaryFunction = f1)
model_rpart<-caret::train(as.factor(isSpam) ~ .,
                          data=emailDFnum, trControl = train_control, 
                          method='rpart',tuneGrid = cart_grid)
model_rpart

cmdf = model_rpart$pred
dt_cm = confusionMatrix(cmdf$pred,cmdf$obs)

```


#### 3.b.ii Random Forest
_Random Forest_ models are an aggregated bootstrap tree-based model. This model type builds ensemble models with simple learning decisions that leave wanting for complexity. Each tree within the decision tree is produced by using the values of an independent set of random vectors that stem from a fixed probability distribution. These models are often described as easy to over-fit however are also highly reliable. _Random Forest_  models are easily and highly interpretable models that other more advanced algorithms are near impossible to interpret in real terms. _List 3.3: Random Forest Hyper Parameter_ contains the hyperparameter that we will tune for an best F1 score with our _random forest_ model.

After cross validated testing we found the Mtry value was 9 with a peak F1 value of __.984__

##### List 3.3: Random Forest Hyper Parameter
Mtry: number of variables randomly sampled as candidates at each split


```{r}
library(randomForest)
rf_grid<-expand.grid(mtry=seq(from =1, to = 25, by = 2))
train_control<-trainControl(method="cv", number=3, 
                            savePredictions = 'final',
                            summaryFunction = f1)
model_rf<-caret::train(as.factor(isSpam) ~ .,
                       data=emailDFnum, trControl = train_control, 
                       ntree=200,method='rf',tuneGrid = rf_grid)
model_rf

cmdf = model_rf$pred
rf_cm = confusionMatrix(cmdf$pred,cmdf$obs)

```



### 3.c XGBoost
We chose to leverage _XGBoost_ (_eXtreme Gradient Boosting_) as our final model for comparison. _XGBoost_ is a sophisticated algorithm that shines when dealing with any irregularities in data. But, while powerful, engineers need to know what hyperparameters to tune in order to get the best model output. The list in _List 3.4: XGBoost Hyper Parameters_ outlines those hyperparameters we chose, what they control, and why we chose them. Those being outlined, there are some general reasons why _XGBoost_ is an effective algorithm to leverage. For instance, there is standard regularization in the _XGBoost_ package. This helps to reduce overfitting and often _XGBoost_ is used as a ‘regularized boosting’ method. Additionally, _XGBoost_ offers parallel processing which allows it to be more economical when it comes to processing power in comparison to the other models. With so many hyper parameters to choose from the _XGBoost_ package is highly flexible and allows for more advanced customization for those engineers who have deep knowledge of the mathematics behind the algorithm. Lastly, things like tree pruning with _max_depth_ in hyperparameters, automatic cross-validation, and the ability to handle missing values _XGBoost_ stands alone as one of the more powerful algorithms to leverage for our research purposes.

Through a cross validated testing run we found the best value of hyperparameters are described below with a F1 score of __.9819__



##### List 3.4: XGBoost Hyper Parameters
Nrounds: Number of boosted trees being fitted to the training data.
Max_depth: controlling the max depth makes this algorithm more conservative. The values can vary depending on the loss function and should always be tuned. This is mainly used to control overfitting as the higher the depth will allow for more models to learn relations for specific samples.
Eta: this is similar to the rate in GBM. The parameter makes the model more robust by decreasing the weights on the individual steps.
Gamma: this identifies the minimum loss reduction necessary to make a split. A node is split solely when the resulting split produces a positive reduction in the loss function. The values can differ all depending on the loss function.

##### Table 3.2: Hyperparameter Settings for XGBoost Model
Hyperparameter      |     Setting       |       F1
----------------------    | ------------------- | ----------------------   
Nrounds              | 100        | .981
MaxDepth           | 11        | 
eta                      | .1        | 
gamma              | 1        | 

```{r}
library(xgboost)
xgb_grid<-expand.grid(nrounds = 100, max_depth = c(3,5,7,9,11), 
                      eta = c(0.01,0.03,0.1), gamma=c(1,3,5,10), 
                      colsample_bytree=1, min_child_weight=1, 
                      subsample=1)
train_control<-trainControl(method="cv", number=3, 
                            savePredictions = 'final',
                            summaryFunction = f1)
model_xgb<-caret::train(as.factor(isSpam) ~ .,data=emailDFnum, 
                        trControl = train_control,method='xgbTree',
                        tuneGrid = xgb_grid)
model_xgb

cmdf = model_xgb$pred
xg_cm = confusionMatrix(cmdf$pred,cmdf$obs)


```


# Results 

The results for our different models by F1 score are below. The data depicts that the Random Forest model is the top performing model over all. While the F1 score is our major score of interest 


##### Table 4.1: Model Results by F1 Score
Hyperparameter      |     F1 Score      
----------------------    | -------------------
Naive Bayes          | 0.9197
RandomForest      | 0.984
DecisionTree         | 0.9608
XGBoost               | 0.9819


##### Naive Bayes

Overall as we expected our Naive Bayes model and approach is the worst model of the four we demonstrated. The model had a best accuracy of about .92 with an average accuracy of .88. We can see with the table the overall confusion matrix is however balanced without a major discrepancy of misclassified spam due to the unbalanced nature of the data we explored earlier. 


```{r}
print(nb_cm)

```

##### Decision Tree Classifier

The decision tree on tis own performed better overall than the Naive Bayes classifier. With an overall accuracy of about .94 across all folds and a peak accuracy of .96 this model worked well but did have some problems with balance. While the Naive Bayes classifier produced errors in a balanced way, we can see with the table below that the Decision Tree had a higher misclassification of spam emails as non spam. 

```{r}
print(dt_cm)

```


##### Random Forest

An overall best model the Random Forest produced average accuracies of .97 with a peak accuracy of .984. We know that a Random Forest model regularly produces good classification results, and it is not surprising as the generalized version and corrected version of the decision tree model it performs a good deal better. We can also see misclassifications are fairly balanced among spam and non-spam emails. 


```{r}
print(rf_cm)

```


##### XGBoost

Finally XGBoost was the last algorithm we ran. It also produced average classification rates of .97 with peak accuracy of .981. As a model it only performed slightly worse than the Random Forest and has many similar characteristics in the model and predictions in the confusion matrix we can see below. 


```{r}
print(xg_cm)

```


# Conclusion

Spam classification is an essential aspect of managing email communication at any time. While we expect the results of these various models to be dated by the origin time of the emails themselves dating back to 2006 and earlier, the techniques may persist as useful ones with more updated lexicons of training emails and spam.

Overall our best models would have been a Random Forest as a better version of the decision tree approach and our XGBoost model. The combination of a Random Forest offering slightly better accuracies with faster run times make this model a slightly more attractive candidate compared to the XGBoost. 

# References

Nolan, D. and Lang, D.T. (2015). Data Science in R A Case Studies Approach to Computational Reasoning and Problem Solving. CRC Press.

Lim, S., & Chi, S. (2019). XGBoost application on bridge management systems for proactive damage estimation. Advanced Engineering Informatics, 41, 100922–. https://doi.org/10.1016/j.aei.2019.100922.




